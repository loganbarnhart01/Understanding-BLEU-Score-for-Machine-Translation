{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Biweekly Report 3 \n",
        "## Understanding and Implementing the BLEU Score Metric\n",
        "\n",
        "### Logan Barnhart"
      ],
      "metadata": {
        "id": "YxIG8t1TJVfB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this report we'll be building some intuition around the BLEU score and implementing it.\n",
        "\n",
        "The BLEU score is an incredibly important metric in the field of machine translation because it served as a baseline sense of accuracy across any given model which would prove valuable when seeing if newer models indeed improved upon older ones. \n",
        "\n",
        "The original paper can be found [here](https://aclanthology.org/P02-1040.pdf)"
      ],
      "metadata": {
        "id": "W3Xl-uDwJeTy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why is BLEU needed over a simple accuracy metric?\n",
        "\n",
        "The primary issue researchers faced was that there are often times many correct translations of any given sentence, so a new metric was needed to adequately handle all the different cases. \n",
        "\n",
        "Additionally, all of the metrics researchers were working with until BLEU was implemented were incredibly expensive and slowed any progress to a hault. "
      ],
      "metadata": {
        "id": "MVO0kupkKVOC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The big idea behind BLEU\n",
        "\n",
        "Typically, the many different but ___on average___ correct translations will share a common vocabulary and some similarity in ordering. The idea is to break the machine translation and reference translations into several sets of n-grams and calculate a weighted average of accuracy across the sets. \n",
        "\n",
        "It turns out that BLEU is not perfect, and may make non-intuitive scores on individual  examples, but on average it makes intuitive scores that would match with a human scorer. \n"
      ],
      "metadata": {
        "id": "2xyKWNemLdjM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Formula - Part 1\n",
        "\n",
        "The researchers use a modified precision which uses the following formula:\n",
        "\n",
        "The score when considering n-gram words in a single sentence, C is: \n",
        "\n",
        "#$p_n = \\frac{\\sum_{\\text{n-gram} \\in C} \\textrm{Count}_{clip}(\\text{n-gram})}{\\sum_{\\text{n-gram} \\in C} \\textrm{Count}(\\text{n-gram})}$\n",
        "\n",
        "where \n",
        "* $\\textrm{Count}(\\text{n-gram})$ is the number of times n-gram occurs in $C$\n",
        "* $\\textrm{Count}_{clip} = \\min \\left ( \\textrm{Count}(\\text{n-gram}), \\textrm{Count}_{ref}(\\text{n-gram}) \\right)$ \n",
        "\n",
        "and \n",
        "\n",
        "* $\\textrm{Count}_{ref}(\\text{n-gram})$ is the largest number of times n-gram occurs in any of the reference sentences. \n",
        "\n",
        "## Example\n",
        "\n",
        "Let's say we are given a sentence in Chinese that translates to some variation of \"The cat is on the mat.\"\n",
        "\n",
        "Machine translation: \n",
        "\n",
        "* The cat the cat the cat.\n",
        "\n",
        "Reference translations:\n",
        "\n",
        "* The cat is on the mat.\n",
        "* There is a cat on the mat.\n",
        "\n",
        "If we're considering unigrams, then \n",
        "\n",
        "$p_1 = \\frac{2 + 1}{3 + 3} = \\frac{1}{2}$\n",
        "\n",
        "since 'the' occurs 2 times in the first reference, 'cat' occurs 1 time in either of the reference sentences, and 'the' and 'cat' both occur 3 times in the translation.\n",
        "\n",
        "Considering bigrams, then \n",
        "\n",
        "$p_2 = \\frac{1}{3 + 2} = \\frac{1}{5}$ since 'the cat' only occurs 1 time in the first reference with 'cat the' not occuring in either reference, while 'the cat' occurs 3 times in the translation, and 'cat the' occurs 2 times in the translation.\n"
      ],
      "metadata": {
        "id": "T04vsAVUDBxg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Formula - Part 2\n",
        "\n",
        "The above method will work on individual sentences, but generally machines will be translating numerous sentences or entire documents. To resolve this the researchers expand the formula by letting \n",
        "\n",
        "#$p_n = \\frac{\\sum_{C \\in \\{sentences\\}} \\sum_{\\text{n-gram} \\in C} \\textrm{Count}_{clip}(\\text{n-gram})}{ \\sum_{C \\in \\{ sentences \\}} \\sum_{\\text{n-gram} \\in C} \\textrm{Count}(\\text{n-gram})}$\n",
        "\n",
        "Where $C$ is a sentence in the set of machine translation sentences. Loosely speaking, this is just a way to average the accuracies of each sentence.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5iv3Cg6A5vnN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Formula - Part 3\n",
        "The researchers wanted a single formula to consider the accuracy across different length n-grams, but while exploring numerically they found a few tendencies of the current scoring system:\n",
        "* shorter sentences tended to score higher even when translations were inaccurate\n",
        "* The score, $p_n$, decayed (roughly) exponentially as n increased\n",
        "\n",
        "To address both issues, they introduced a 'brevity penalty' (BP) and would do a weighted sum of the logs of the scores across several n-grams, because of this their final formula is:\n",
        "\n",
        "# $\\textrm{BLEU} = \\textrm{BP} \\cdot \\exp \\left( \\sum_{n = 1}^N w_n \\log(p_n) \\right)$\n",
        "\n",
        "To calculate BP, we let \n",
        "\n",
        "* $c$ be the length of the translation \n",
        "\n",
        "and \n",
        "\n",
        "* $r$ be the 'effective reference length' which is calculated by summing the lengths of the reference sentences that are closest in length to the machine translation they would correlate to \n",
        "\n",
        "(i.e. if our machine-translated document was \"Hello. Goodbye.\" and our set of reference translations for the first and second sentences respectively was {\"Hi.\", \"Hey There.\"} and {\"See you.\", \"Until next time.\"} then $c = 2$ and $r = 1 + 2$)\n",
        "\n",
        "Then\n",
        "\n",
        "#$BP = \n",
        "  \\begin{cases}\n",
        "    1 & \\text{if } c > r\\\\\n",
        "    \\exp(1 - \\frac{r}{c}) & \\text{if } c \\leq r\n",
        "  \\end{cases}\n",
        "$\n",
        "\n",
        "The researchers used a baseline of $N = 4, w_n = \\frac{1}{N} = \\frac{1}{4}$"
      ],
      "metadata": {
        "id": "XDsN05jkYRbH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Implementation\n",
        "\n",
        "## $\\text{Count}$ and $\\text{Count}_{clip}$\n",
        "\n",
        "First, we'll define some functions to calculate the $\\text{Count}$ and $\\text{Count}_{clip}$:"
      ],
      "metadata": {
        "id": "BbWgSSGgs3KC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "\n",
        "### n_gram: string representing an n-gram\n",
        "### translation: list strings of all n-grams in the machine translation\n",
        "def count(n_gram, translation):\n",
        "  return np.sum(np.array(translation) == n_gram)\n",
        "\n",
        "### n_gram: string representing an n-gram\n",
        "### refs: list of lists containing strings, the n-grams in each reference\n",
        "def count_ref(n_gram, refs):\n",
        "  max_count = 0\n",
        "  for ref in refs:\n",
        "    rcount = np.sum(np.array(ref) == n_gram)\n",
        "    if rcount > max_count:\n",
        "      max_count = rcount\n",
        "  return max_count\n",
        "\n",
        "### n_gram: string representing an n-gram \n",
        "### translation: list of all n-grams in the machine translation\n",
        "### refs: list of lists containing strings, the n-grams in each reference\n",
        "def count_clip(n_gram, translation, refs):\n",
        "  return min(count(n_gram, translation), count_ref(n_gram, refs))"
      ],
      "metadata": {
        "id": "CKh7t1NRAFvj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## $p_n$ for single sentences\n",
        "\n",
        "Now, let's code a function to calculate $p_n$ for single sentence translations and then we'll generalize entire translated documents."
      ],
      "metadata": {
        "id": "0-HWBuNttHDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculates pn for single sentences\n",
        "\n",
        "### translation: a string machine translation\n",
        "### refs: a list of strings of reference translations\n",
        "### n: int, n-gram length \n",
        "def pn_single(translation, refs, n):\n",
        "\n",
        "  translation = translation.lower() # don't care about capitalization \n",
        "  translation = re.sub(r'[^\\w\\s]', '',translation) # remove punctuation\n",
        "  translation = translation.split(' ') # split at spaces\n",
        "  \n",
        "  translation_n_grams = []\n",
        "\n",
        "  #creating list of n-grams in translation\n",
        "  for i in range(len(translation) - n + 1):\n",
        "    translation_n_grams.append(' '.join(translation[i : i + n]))\n",
        "\n",
        "  #processing reference sentences\n",
        "  for idx, ref in enumerate(refs):\n",
        "    ref = ref.lower() \n",
        "    ref = re.sub(r'[^\\w\\s]', '',ref) \n",
        "    refs[idx] = ref.split(' ')\n",
        "  \n",
        "  ref_n_grams_full = []\n",
        "  #creating list of lists of n-grams in the references\n",
        "  for ref in refs:\n",
        "    ref_n_grams = []\n",
        "    for i in range(len(ref) - n + 1):\n",
        "      ref_n_grams.append(' '.join(ref[i : i + n]))\n",
        "    ref_n_grams_full.append(ref_n_grams)\n",
        "  \n",
        "  numerator = 0\n",
        "  denominator = 0\n",
        "\n",
        "  evaluated_n_grams = []\n",
        "\n",
        "  for ng in translation_n_grams:\n",
        "    if ng not in evaluated_n_grams: # we don't want to evaluate the same n-gram twice, but we want to count all occurrences\n",
        "      numerator += count_clip(ng, translation_n_grams, ref_n_grams_full)\n",
        "      denominator += count(ng, translation_n_grams)\n",
        "      evaluated_n_grams.append(ng)\n",
        "\n",
        "  return numerator/denominator"
      ],
      "metadata": {
        "id": "AgCIUv1f3VEl"
      },
      "execution_count": 317,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "machine_translation = 'The cat the cat the cat.'\n",
        "references = ['The cat is on the mat.', 'There is a cat on the mat']\n",
        "\n",
        "pn_single(machine_translation, references, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SIjCmnCYRVo",
        "outputId": "a3965920-4fe6-42cc-df16-7ee1f8be456b"
      },
      "execution_count": 318,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {},
          "execution_count": 318
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That returns the correct value we calculated earlier!\n",
        "\n",
        "## $p_n$ for multi-sentence strings\n",
        "\n",
        "Now I'll implement the generalized version for multi-sentence documents. First I'm going to create an n-gram helper. This just returns the lists containing the n-grams for the translation and references just like for `pn_single` but it is slightly more nested and ugly. \n",
        "\n",
        "I didn't have to make a helper function and could have just left it inside the `pn` function, but I thought I would need this for the brevity penalty, although it turned out I did not!"
      ],
      "metadata": {
        "id": "wKMpBRvgtiR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns lists of n-grams for the translations and references\n",
        "\n",
        "def n_gram_helper(translations, refs_full, n):\n",
        "  t = translations.lower() # don't care about capitalization \n",
        "  t = t.split('.')\n",
        "  t = t[:-1] # to remove a trailing '' in list\n",
        "\n",
        "  for idx, translation in enumerate(t): \n",
        "    translation = translation.strip()\n",
        "    translation = re.sub(r'[^\\w\\s]', '',translation) # remove punctuation\n",
        "    translation = translation.split(' ') # split at spaces\n",
        "    t[idx] = translation\n",
        "  \n",
        "  translation_n_grams_full = []\n",
        "  for translation in t:\n",
        "    translation_n_grams = []\n",
        "    for i in range(len(translation) - n + 1):\n",
        "      translation_n_grams.append(' '.join(translation[i : i + n]))\n",
        "    translation_n_grams_full.append(translation_n_grams)\n",
        "\n",
        "  #processing reference sentences\n",
        "  r_full = []\n",
        "  for idx1, refs in enumerate(refs_full): \n",
        "    rs = []\n",
        "    for idx2, ref in enumerate(refs):\n",
        "      r = ref.lower() \n",
        "      r = re.sub(r'[^\\w\\s]', '',r) \n",
        "      rs.append(r.split(' '))\n",
        "    r_full.append(rs)\n",
        "  \n",
        "  ref_n_grams_full = []\n",
        "  for refs in r_full:\n",
        "    ref_n_grams_mid = []\n",
        "    for ref in refs:\n",
        "      ref_n_grams = []\n",
        "      for i in range(len(ref) - n + 1):\n",
        "        ref_n_grams.append(' '.join(ref[i : i + n]))\n",
        "      ref_n_grams_mid.append(ref_n_grams)\n",
        "    ref_n_grams_full.append(ref_n_grams_mid)\n",
        "\n",
        "  return translation_n_grams_full, ref_n_grams_full"
      ],
      "metadata": {
        "id": "ZO93iRI0BWud"
      },
      "execution_count": 265,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculates pn for entire documents, returns both pn and brevity penalty for fewer computations.\n",
        "\n",
        "### translation: string containing all translated sentences\n",
        "### refs: list of lists containing the possible reference translations for each sentence in order. \n",
        "### n : int, n-gram length. \n",
        "def pn(translations, refs_full, n):\n",
        "  translation_n_grams_full, ref_n_grams_full = n_gram_helper(translations, refs_full, n)\n",
        "  # print(translation_n_grams_full)\n",
        "  # print(ref_n_grams_full)\n",
        "\n",
        "  numerator = 0\n",
        "  denominator = 0\n",
        "\n",
        "  for idx, translation in enumerate(translation_n_grams_full):\n",
        "    evaluated_n_grams = []\n",
        "    for ng in translation:\n",
        "      if ng not in evaluated_n_grams: # we don't want to evaluate the same n-gram twice, but we want to count all occurrences\n",
        "        numerator += count_clip(ng, translation, ref_n_grams_full[idx])\n",
        "        denominator += count(ng, translation)\n",
        "        evaluated_n_grams.append(ng)\n",
        "\n",
        "  return numerator/denominator"
      ],
      "metadata": {
        "id": "DZYS3SutsMAL"
      },
      "execution_count": 241,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If this works then according to the formula, the correct pn for two sentences that are identical to our earlier example should be \n",
        "$(3 + 3)/(6 + 6) = .5$ still. "
      ],
      "metadata": {
        "id": "Lsc0dE4i1nGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test = 'The cat the cat the cat. The cat the cat the cat.'\n",
        "refs = [['The cat is on the mat.', 'There is a cat on the mat'], ['The cat is on the mat.', 'There is a cat on the mat']]\n",
        "pn(test, refs, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgRxGJ4kui2G",
        "outputId": "1efe18f8-421f-4d3e-9138-9b116fcbb131"
      },
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {},
          "execution_count": 224
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And the following example should yield (2 + 3) / (7 + 6)\n",
        "\n"
      ],
      "metadata": {
        "id": "fnSU1TcI33Nk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test = 'The the the the the the the. The cat the cat the cat.'\n",
        "refs = [['The cat is on the mat.', 'There is a cat on the mat'], ['The cat is on the mat.', 'There is a cat on the mat']]\n",
        "pn(test, refs, 1)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGBhBdcSutHy",
        "outputId": "9d411baa-59a8-4f95-a215-d2c41d460fbe"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.38461538461538464"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "5/13"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0jmpu9e0-2t",
        "outputId": "83ade2ac-4ab9-4bd0-b328-c27b01e5abc9"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.38461538461538464"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Smashing, now we can implement the brevity penalty. \n",
        "\n",
        "## Brevity Penalty\n",
        "\n",
        "We just need to calculate the length of the translation and for each sentence find the reference that's closest in length. "
      ],
      "metadata": {
        "id": "JC40qizbPRCN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# returns brevity penalty\n",
        "\n",
        "### translation: list of all the words in a translation\n",
        "### refs: list of lists containing all words in a reference translation\n",
        "def brevity_penalty(translation, refs):\n",
        "  c = 0 \n",
        "  r = 0\n",
        "  sentences = translation.split('.')\n",
        "  sentences.remove('')\n",
        "  for idx, s in enumerate(sentences):\n",
        "    s_split = s.strip().split(' ')\n",
        "    t_sentence_length = len(s_split)\n",
        "    c += t_sentence_length # sum translation lengths \n",
        "    \n",
        "    # finding the best reference length\n",
        "    best_ref_length = float('inf') # start at infinity so first reference satisfies inequality below\n",
        "    for ref in refs[idx]:\n",
        "      if abs(len(ref.strip().split(' ')) - t_sentence_length) < abs(t_sentence_length - best_ref_length): # Check distance between new and old ref length - translation \n",
        "        best_ref_length = len(ref.strip().split(' '))\n",
        "\n",
        "    r += best_ref_length # summing best ref lengths to get effective ref length, r\n",
        "\n",
        "  if c > r:\n",
        "    return 1\n",
        "  else:\n",
        "    return np.exp(1 - r/c)"
      ],
      "metadata": {
        "id": "gq6SS4piE3Nz"
      },
      "execution_count": 319,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check that it works I'm going to do an example where the translation length exactly matches the reference lengths -> bp = 1, and an example that's shorter -> bp < 1"
      ],
      "metadata": {
        "id": "m_pmeO1fZs-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Should yield BP = 1\n",
        "test = 'The cat the cat the cat. There is not a dog door.'\n",
        "refs = [['The cat is on the mat', 'There is a cat on the mat.'], ['The dog is at the door', 'There is a dog at the door']]\n",
        "\n",
        "brevity_penalty(test, refs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNqGV-AEZqlC",
        "outputId": "7036b473-3af6-46dc-f837-a13f905caf02"
      },
      "execution_count": 320,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 320
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Should yield BP < 1\n",
        "test = 'cat. dog.'\n",
        "refs = [['The cat is on the mat', 'There is a cat on the mat.'], ['The dog is at the door', 'There is a dog at the door']]\n",
        "\n",
        "brevity_penalty(test, refs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxVvn-d-aKkc",
        "outputId": "ebc8f772-5357-4a9c-9031-4ced9fe054de"
      },
      "execution_count": 323,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.006737946999085467"
            ]
          },
          "metadata": {},
          "execution_count": 323
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Very good, the second example is *much* shorter than the references so thankfully the bp is quite small. \n",
        "\n",
        "\n",
        "## BLEU Score\n",
        "Finally, let's put it all together and get a working BLEU score:"
      ],
      "metadata": {
        "id": "NduuocI_4NG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculates BLEU score with uniform weights\n",
        "\n",
        "def BLEU_score(translation, refs, N, wn):   \n",
        "  sum = 0\n",
        "  bp = brevity_penalty(translation, refs)\n",
        "\n",
        "  for n in range(1, N + 1):\n",
        "    p = pn(translation, refs, n)\n",
        "    sum += wn * np.log(p)\n",
        "    \n",
        "  return bp * np.exp(sum)\n"
      ],
      "metadata": {
        "id": "3Cl2ncmp7hlL"
      },
      "execution_count": 315,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = 'The cat the cat the cat. There is a dog door.'\n",
        "refs = [['The cat is on the mat', 'There is a cat on the mat.'], ['The dog is at the door', 'There is a dog at the door']]\n",
        "\n",
        "BLEU_score(test, refs, 4, .25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55f0jMKl_icH",
        "outputId": "1722aa2b-4aeb-44b8-87ee-7566570d4d12"
      },
      "execution_count": 316,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3366184122712009"
            ]
          },
          "metadata": {},
          "execution_count": 316
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There we go, we have a score and since the first sentence is almost entirely wrong - it makes sense the score is pretty low. "
      ],
      "metadata": {
        "id": "XUyfiNSrWOhQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusions\n",
        "\n",
        "I'm actually a bit iffy on the current state of my implementation. Going through the paper for the BLEU scoring system was a bit of a rollercoaster - the notation was hard to keep clear for me and I was originally uncomfortable with it, then after reading it again and starting implementation thought I had figured it out, but now I'm uncomfortable again, here's why: \n",
        "\n",
        "## Uh oh...\n",
        "\n",
        "What if we set N = 5? Well, there's no 5-gram in either sentence that matches any reference, so $p_5 = 0$ must be true. But if $p_5 = 0$, then we end up adding $\\log(0)$ to the sum, well... I think we all know that's not going to work out. \n",
        "\n",
        "On its own, if we got $e^{\\log(0)}$ that would be fine, because it would make sense, if $p_n = 0$  then of course our idea of accuracy should be 0. But using the example above that's not quite the case because we have some level of accuracy for 1,2,3, and 4-grams. \n",
        "\n",
        "## Why might this be happening?\n",
        "\n",
        "At the moment I am unsure if my interpretation of their notation / formulas is incorrect, or if this is an intended behavior of the scoring system, perhaps this is why they only did up to 4-grams as their baseline, because if there's any sentence with fewer than $N$ n-grams then the BLEU score will always be 0.\n",
        "\n",
        "I thought it may have been my $p_n$ code since it's the most involved, but after running this on some more complex examples directly from the paper I'm actually getting the same values as the researchers! So that may not be the case.\n",
        "\n",
        "I'm still leaning towards my understanding being wrong, which is unfortunate, but less catastrophic than the thought of BLEU having such a major flaw. If you spot what I did wrong, please let me know! This will keep me up for nights to come.... \n",
        "\n",
        "## Final Remarks\n",
        "\n",
        "I was hoping to load a translation model and mess around to see how well my scores lined up with scores in literature, but its quite late so that will not be happening this week!\n",
        "\n",
        "Also, as a final note, don't use my implementation if you want to do BLEU scoring on a translation model - there are more efficient libraries online and I was just implementing as a way to become more familiar with the scoring system. \n"
      ],
      "metadata": {
        "id": "i6_mYFTRTr9-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4aOKa3APbWV8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}